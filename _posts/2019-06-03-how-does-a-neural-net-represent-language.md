---
layout: post
date: "2019-06-03"
author: "Prathyush SP"
link: "https://pair-code.github.io/interpretability/bert-tree/"
type: "Paper"
title: "How does a neural net represent language?"
tags: ""
comments: true
---
Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT

https://arxiv.org/abs/1906.02715