---
layout: post
date: "2019-07-09"
author: "Prathyush SP"
link: "https://github.com/DSE-MSU/R-transformer

https://arxiv.org/abs/1907.05572"
type: "Paper"
title: "R-Transformer - Maybe attention isn't *all* we need."
tags: ""
comments: true
---
Recurrent Neural Networks have long been the dominating choice for sequence modeling. However, it severely suffers from two issues: impotent in capturing very long-term dependencies and unable to parallelize the sequential computation procedure. Therefore, many non-recurrent sequence models that are built on convolution and attention operations have been proposed recently. Notably, models with multi-head attention such as Transformer have demonstrated extreme effectiveness in capturing long-term dependencies in a variety of sequence modeling tasks.