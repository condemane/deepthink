---
layout: post
date: "2019-02-02"
author: "Prathyush SP"
link: "https://arxiv.org/abs/1901.09321"
category: "Paper"
title: "Fixup Initialization: Residual Learning Without Normalization"
tags: ""
comments: true
---
They manage to train deep nets (10k layers!) w/o BatchNorm, by careful init scaling & initializing the 2nd residual conv to 0.